{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Flood Mapping Explained\n",
    "\n",
    "TODO:\n",
    "- make setup with EODC Dask Gateway work\n",
    "- Simplify notebook (aggregate functions, improve descriptions, make it smaller/concise)\n",
    "Maybe sth like this:\n",
    "   ``` \n",
    "    sig0_dc, hpar_dc, plia_dc = preprocess(bbox, datetime, dynamic)\n",
    "    flood_dc = calculate_flood_dc(sig0_dc, plia_dc, hpar_dc)\n",
    "    flood_dc[\"wbsc\"] = calc_water_likelihood(flood_dc)  # Water\n",
    "    flood_dc[\"hbsc\"] = harmonic_expected_backscatter(flood_dc)  # Land\n",
    "    ```\n",
    "- Currently access to Dask EODC Gateway is only granted after requesting additional permission via e-mail\n",
    "- Add already a predefined number of workers, so the user doesnt need to worry about it\n",
    "- See if EODC Dask Gateway can handle the amount of participants\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809b7646",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install dask-flood-mapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfc71d2",
   "metadata": {},
   "source": [
    "## Set connection to EODC Dask Gateway\n",
    "Autentication is required through an username and password that should be requested to EODC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827c1ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"AWS_NO_SIGN_REQUEST\"] = \"YES\"\n",
    "\n",
    "\n",
    "import hvplot.xarray  # noqa\n",
    "from dask_flood_mapper import flood\n",
    "from eodc import settings\n",
    "from eodc.dask import EODCDaskGateway\n",
    "from rich.console import Console\n",
    "from rich.prompt import Prompt\n",
    "import os\n",
    "import numpy as np\n",
    "import pystac_client\n",
    "import xarray as xr\n",
    "from dask.distributed import Client, wait\n",
    "from odc import stac as odc_stac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f701930",
   "metadata": {},
   "outputs": [],
   "source": [
    "settings.DASK_URL = \"http://dask.services.eodc.eu\"\n",
    "settings.DASK_URL_TCP = \"tcp://dask.services.eodc.eu:10000/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "console = Console()\n",
    "your_username = Prompt.ask(prompt=\"Enter your Username\")\n",
    "gateway = EODCDaskGateway(username=your_username)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef9021b",
   "metadata": {},
   "source": [
    "### Cluster Configuration\n",
    "\n",
    "It is possible to change the default configuration to optimize your specifications at the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37eb5063",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_options = gateway.cluster_options()\n",
    "cluster_options.image = \"ghcr.io/eodcgmbh/cluster_image:2025.4.1\"\n",
    "cluster_options.worker_cores = 8\n",
    "cluster_options.worker_memory = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c2fa79",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Important:</b> Per default no worker is spawned, therefore please use the widget to add/scale Dask workers in order to enable computations on the cluster.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd51de69",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = gateway.new_cluster(cluster_options)\n",
    "cluster.scale(5) # Or adaptative scale: cluster.adapt(minimum=3, maximum=8)\n",
    "client = cluster.get_client()\n",
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf06cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# command to check compatibility between client and scheduler\n",
    "versions = client.get_versions(check=True)\n",
    "\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(versions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d426181",
   "metadata": {},
   "source": [
    "#### List available clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44dadaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "console.print(gateway.list_clusters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92563f9f",
   "metadata": {},
   "source": [
    "#### Connect to already running clusters \n",
    "\n",
    "Instead of restarting the whole process again, you can directly connect to cluster already running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "646ab149",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cluster = gateway.connect(gateway.list_clusters()[0].name)\n",
    "#console.print(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a02e9ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cluster.close(shutdown=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5f217f",
   "metadata": {},
   "source": [
    "### Dask dashboard\n",
    "\n",
    "The following link displays the Dask Dashboard that can be used to monitor the execution of computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2614603a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.dashboard_link "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "In conjunction with setting up the Dask client qw will chunk the arrays along three dimensions according to the following specifications for maximum performance on my setup. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = {\"time\": 1, \"latitude\": 1300, \"longitude\": 1300}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Cube Definitions\n",
    "\n",
    "The following generic specifications are used for presenting the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coordinate Reference System - World Geodetic System 1984 (WGS84) in this case\n",
    "crs = \"EPSG:4326\"\n",
    "res = 0.00018  # 20 meter in degree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Northern Germany Flood\n",
    "\n",
    "Storm Babet hit the Denmark and Northern coast at the 20th of October 2023 [Wikipedia](https://en.wikipedia.org/wiki/Storm_Babet). Here an area around Zingst at the Baltic coast of Northern Germany is selected as the study area.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_range = \"2023-10-11/2023-10-25\"\n",
    "minlon, maxlon = 12.3, 13.1\n",
    "minlat, maxlat = 54.3, 54.6\n",
    "bounding_box = [minlon, minlat, maxlon, maxlat]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## EODC STAC Catalog\n",
    "\n",
    "The `pystac_client` establishes a connection to the EODC STAC Catalog. This results in a catalog object that can be used to discover collections and items hosted at EODC.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "eodc_catalog = pystac_client.Client.open(\"https://stac.eodc.eu/api/v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360b393c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pystac_client import Client\n",
    "\n",
    "# Open the STAC API\n",
    "#client = Client.open(\"https://stac.eodc.eu/api/v1\")\n",
    "\n",
    "# Get all collections\n",
    "collections = eodc_catalog.get_collections()\n",
    "\n",
    "# Print each collection ID and title (if available)\n",
    "for collection in collections:\n",
    "    print(f\"ID: {collection.id}\")\n",
    "    if collection.title:\n",
    "        print(f\"Title: {collection.title}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Microwave Backscatter Measurements\n",
    "\n",
    "The basic premise of microwave-based backscattering can be seen in the sketch below, the characteristics of backscattering over land and water differ considerably. With this knowledge we can detect whenever a pixel with a predominant land like signature changes to a water like signature in the event of flooding.\n",
    "\n",
    "![](https://www.gsi.ie/images/images/SAR_mapping_land_water.jpg)\n",
    "\n",
    "*Schematic backscattering over land and water. Image from [Geological Survey Ireland](https://www.gsi.ie/images/images/SAR_mapping_land_water.jpg)*\n",
    "\n",
    "We discover Sentinel-1 microwave backscatter ($\\sigma_0$ [1]) at a 20 meter resolution, like so:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "search = eodc_catalog.search(\n",
    "    collections=\"SENTINEL1_SIG0_20M\",\n",
    "    bbox=bounding_box,\n",
    "    datetime=time_range,\n",
    ")\n",
    "\n",
    "items_sig0 = search.item_collection()\n",
    "items_sig0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "The state of the orbit and relative orbit number is also saved, as the water and land likelihoods (which are calculated later on) dependent on the orbital configuration. These variables will be added as additional coordinates to the data cube. For this purpose a small helper function is defined, like so:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_orbit_names(items):\n",
    "    return np.array(\n",
    "        [\n",
    "            items[i].properties[\"sat:orbit_state\"][0].upper()\n",
    "            + str(items[i].properties[\"sat:relative_orbit\"])\n",
    "            for i in range(len(items))\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "We will also save the scaling factor and nodata values of STAC items to correct the loaded data accordingly. Again a helper function will be used to correctly scale and fill no data values of the cube.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process_eodc_cube(dc: xr.Dataset, items, bands):\n",
    "    if not isinstance(bands, tuple):\n",
    "        bands = tuple([bands])\n",
    "    for i in bands:\n",
    "        dc[i] = post_process_eodc_cube_(\n",
    "            dc[i], items, i\n",
    "        )  # https://github.com/TUW-GEO/dask-flood-mapper.git\n",
    "    return dc\n",
    "\n",
    "\n",
    "def post_process_eodc_cube_(dc: xr.Dataset, items, band):\n",
    "    scale = items[0].assets[band].extra_fields.get(\"raster:bands\")[0][\"scale\"]\n",
    "    nodata = items[0].assets[band].extra_fields.get(\"raster:bands\")[0][\"nodata\"]\n",
    "    return dc.where(dc != nodata) / scale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "The VV polarization of the discover items can be loaded with `odc-stac` and cast in the desired projection and resolution. The data is at this point only lazily loaded, meaning that we only make an instance of the outlines of the datacube with the proper shape and resolution, but without actually loading all the data. This is done by providing the chunks as defined before, which partitions the data in portions which are more easily handled by the setup used for processing the data (in this case my own pc). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "bands = \"VV\"\n",
    "sig0_dc = odc_stac.load(\n",
    "    items_sig0,\n",
    "    bands=bands,\n",
    "    crs=crs,\n",
    "    chunks=chunks,\n",
    "    resolution=res,\n",
    "    bbox=bounding_box,\n",
    "    resampling=\"bilinear\",\n",
    "    groupby=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "Now we can rescale our variable, fill the no data values with `np.nan` values, and add the orbit names, with the previous defined functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "sig0_dc = (\n",
    "    post_process_eodc_cube(sig0_dc, items_sig0, bands)\n",
    "    .rename_vars({\"VV\": \"sig0\"})\n",
    "    .assign_coords(orbit=(\"time\", extract_orbit_names(items_sig0)))\n",
    "    .dropna(dim=\"time\", how=\"all\")\n",
    "    .sortby(\"time\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "Then we remove duplicate time dimensions from the data cube and extract the orbit names as we will need those for obtaining the correct harmonic parameters and local incidence angles,as explained in the next section. Also, note, that we call `dask.persist` to materialize the object but retain it as a delayed object in the workers memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "__, indices = np.unique(sig0_dc.time, return_index=True)\n",
    "indices.sort()\n",
    "orbit_sig0 = sig0_dc.orbit[indices].data\n",
    "sig0_dc = sig0_dc.groupby(\"time\").mean(skipna=True)\n",
    "sig0_dc = sig0_dc.assign_coords(orbit=(\"time\", orbit_sig0))\n",
    "sig0_dc = sig0_dc.persist()\n",
    "sig0_dc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## Harmonic Parameters\n",
    "\n",
    "The so-called likelihoods of $P(\\sigma^0|flood)$ and $P(\\sigma^0|nonflood)$ can be calculated from past backscattering information. To be able to this we load the harmonic parameters we can model the expected variations in land back scattering based on seasonal changes in vegetation. The procedure is similar to the backscattering routine.\n",
    "\n",
    "We discover items.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "search = eodc_catalog.search(collections=\"SENTINEL1_HPAR\", bbox=bounding_box)\n",
    "\n",
    "items_hpar = search.item_collection()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "Load the data as a lazy object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "bands = (\"C1\", \"C2\", \"C3\", \"M0\", \"S1\", \"S2\", \"S3\", \"STD\")\n",
    "hpar_dc = odc_stac.load(\n",
    "    items_hpar,\n",
    "    bands=bands,\n",
    "    crs=crs,\n",
    "    chunks=chunks,\n",
    "    resolution=res,\n",
    "    bbox=bounding_box,\n",
    "    groupby=None,\n",
    ")\n",
    "\n",
    "hpar_dc = post_process_eodc_cube(hpar_dc, items_hpar, bands).rename({\"time\": \"orbit\"})\n",
    "hpar_dc[\"orbit\"] = extract_orbit_names(items_hpar)\n",
    "hpar_dc = hpar_dc.groupby(\"orbit\").mean(skipna=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "We expand the variables along the orbits of sigma naught to be able to calculate the correct land reference backscatter signatures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "hpar_dc = hpar_dc.sel(orbit=orbit_sig0)\n",
    "hpar_dc = hpar_dc.persist()\n",
    "hpar_dc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "## Local Incidence Angles\n",
    "\n",
    "Local incidence angles of measured microwave backscattering is as well important for calculating reference backscatter values, but now for water bodies. The procedure is much the same as for the harmonic parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "search = eodc_catalog.search(collections=\"SENTINEL1_MPLIA\", bbox=bounding_box)\n",
    "\n",
    "items_plia = search.item_collection()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "Load the lazy object and preprocess.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "bands = \"MPLIA\"\n",
    "plia_dc = odc_stac.load(\n",
    "    items_plia,\n",
    "    bands=bands,\n",
    "    crs=crs,\n",
    "    chunks=chunks,\n",
    "    resolution=res,\n",
    "    bbox=bounding_box,\n",
    "    groupby=None,\n",
    ")\n",
    "\n",
    "plia_dc = post_process_eodc_cube(plia_dc, items_plia, bands).rename({\"time\": \"orbit\"})\n",
    "plia_dc[\"orbit\"] = extract_orbit_names(items_plia)\n",
    "plia_dc = plia_dc.groupby(\"orbit\").mean(skipna=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "We expand the variables along the orbits of sigma naught to be able to calculate the correct land reference backscatter signatures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "plia_dc = plia_dc.sel(orbit=orbit_sig0)\n",
    "plia_dc = plia_dc.persist()\n",
    "plia_dc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "## ESA World Cover from Terrascope\n",
    "\n",
    "For flood mapping we are only interested in microwave backscattering over what used to be land, as such, we need a way to mask water bodies. For this we use the ESA World Cover data from the following STAC catalog.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"AWS_NO_SIGN_REQUEST\"] = \"YES\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "Similarly, we discover the required items.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb198484",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_GFM = eodc_catalog.search(\n",
    "    collections=\"GFM\",\n",
    "    bbox=bounding_box,\n",
    "    datetime=\"2023-10-11T05:33:43.000000000\", # choose one time stamp for simplification an avoid errors\n",
    ")\n",
    "\n",
    "items_GFM = search_GFM.item_collection()\n",
    "items_GFM\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6f08e643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resoluton and crs are different from the rest of the data cubes, wll that be a problem to merge?\n",
    "\n",
    "#resolution = items_GFM[0].properties['gsd']\n",
    "#print(resolution)\n",
    "\n",
    "#import pyproj\n",
    "#crs = pyproj.CRS.from_wkt(items_GFM[0].properties[\"proj:wkt2\"])\n",
    "#print(crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9ff551fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "GFM_dc= (odc_stac.load(\n",
    "    items_GFM, \n",
    "    bbox=bounding_box,   # Define the bounding box for the area of interest\n",
    "    crs=crs,   # Set the coordinate reference system\n",
    "    bands=[\"reference_water_mask\"],   # Specify the bands to load, comment to load all bands\n",
    "    resolution=res   # Set the resolution of the data\n",
    "    #dtype='uint8',   # Define the data type\n",
    "    #chunks={\"x\": 1000, \"y\": 1000, \"time\": -1},  # Set the chunk size for Dask\n",
    "    )\n",
    "    .squeeze(\"time\")\n",
    "    .drop_vars(\"time\")\n",
    "    .rename_vars({\"reference_water_mask\": \"wcover\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da40a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "GFM_dc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "Load the data lazily.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "## Fuse cube\n",
    "\n",
    "Here we fuse the four data cubes together and filter for the values that have a HAND value of above zero. We can now drop the obit coordinates as well as time slices which contain no land backscattering data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "340134fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"AWS_NO_SIGN_REQUEST\"] = \"YES\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"AWS_NO_SIGN_REQUEST\"] = \"YES\"\n",
    "\n",
    "import xarray as xr\n",
    "\n",
    "flood_dc = xr.merge([sig0_dc, plia_dc, hpar_dc, GFM_dc])\n",
    "flood_dc = flood_dc.where(flood_dc.wcover != 255)\n",
    "\n",
    "\n",
    "flood_dc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5332e211",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"AWS_NO_SIGN_REQUEST\"] = \"YES\"\n",
    "\n",
    "from odc.stac import configure_rio\n",
    "configure_rio(aws={\"aws_unsigned\": True})\n",
    "\n",
    "\n",
    "flood_dc = (\n",
    "    flood_dc.reset_index(\"orbit\", drop=True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1d15b67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "flood_dc = flood_dc.rename({\"orbit\": \"time\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "32fa9d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"AWS_NO_SIGN_REQUEST\"] = \"YES\"\n",
    "\n",
    "from odc.stac import configure_rio\n",
    "configure_rio(aws={\"aws_unsigned\": True})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "30ac3538",
   "metadata": {},
   "outputs": [],
   "source": [
    "flood_dc = flood_dc.dropna(dim=\"time\", how=\"all\", subset=[\"sig0\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "## Likelihoods\n",
    "\n",
    "Now we are ready to calculate the likelihoods of micorwave backscattering given flooding (or non flooding).\n",
    "\n",
    "### Water\n",
    "\n",
    "We start with water which is the simplest calculation, where the hard coded values are coefficients of a regression model fitted to global water backscattering values.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_water_likelihood(dc):\n",
    "    return dc.MPLIA * -0.394181 + -4.142015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "flood_dc[\"wbsc\"] = calc_water_likelihood(flood_dc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "### Land\n",
    "\n",
    "For land backscattering we construct the harmonic model from the parameters as contained in the fused data cube. By doing so, we obtain a reference land backscattering value to which to compare our actual observed sigma naught values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def harmonic_expected_backscatter(dc):\n",
    "    w = np.pi * 2 / 365\n",
    "\n",
    "    t = dc.time.dt.dayofyear\n",
    "    wt = w * t\n",
    "\n",
    "    M0 = dc.M0\n",
    "    S1 = dc.S1\n",
    "    S2 = dc.S2\n",
    "    S3 = dc.S3\n",
    "    C1 = dc.C1\n",
    "    C2 = dc.C2\n",
    "    C3 = dc.C3\n",
    "    hm_c1 = (M0 + S1 * np.sin(wt)) + (C1 * np.cos(wt))\n",
    "    hm_c2 = (hm_c1 + S2 * np.sin(2 * wt)) + C2 * np.cos(2 * wt)\n",
    "    hm_c3 = (hm_c2 + S3 * np.sin(3 * wt)) + C3 * np.cos(3 * wt)\n",
    "    return hm_c3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "flood_dc[\"hbsc\"] = harmonic_expected_backscatter(flood_dc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "## Flood mapping\n",
    "\n",
    "Having calculated the likelihoods, we can now move on to calculate the probability of (non-)flooding given a pixel's $\\sigma^0$. These so-called *posteriors* need one more piece of information, as can be seen in the equation above. We need the probability that a pixel is flooded $P(F)$ or not flooded $P(NF)$. Of course, these are the figures we've been trying to find this whole time. We don't actually have them yet, so what can we do? In Bayesian statistics, we can just start with our best guess. These guesses are called our \"priors\", because they are the beliefs we hold *prior* to looking at the data. This subjective prior belief is the foundation Bayesian statistics, and we use the likelihoods we just calculated to update our belief in this particular hypothesis. This updated belief is called the \"posterior\".\n",
    "\n",
    "Let's say that our best estimate for the chance of flooding versus non-flooding of a pixel is 50-50: a coin flip.  We now can also calculate the probability of backscattering $P(\\sigma^0)$, as the weighted average of the water and land likelihoods, ensuring that our posteriors range between 0 to 1.\n",
    "\n",
    "The following code block shows how we calculate the priors which allow use to predict whether it is likely if a land pixel became flooded.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayesian_flood_decision(dc):\n",
    "    nf_std = 2.754041\n",
    "    sig0 = dc.sig0\n",
    "    std = dc.STD\n",
    "    wbsc = dc.wbsc\n",
    "    hbsc = dc.hbsc\n",
    "\n",
    "    f_prob = (1.0 / (std * np.sqrt(2 * np.pi))) * np.exp(\n",
    "        -0.5 * (((sig0 - wbsc) / nf_std) ** 2)\n",
    "    )\n",
    "    nf_prob = (1.0 / (nf_std * np.sqrt(2 * np.pi))) * np.exp(\n",
    "        -0.5 * (((sig0 - hbsc) / nf_std) ** 2)\n",
    "    )\n",
    "\n",
    "    evidence = (nf_prob * 0.5) + (f_prob * 0.5)\n",
    "    nf_post_prob = (nf_prob * 0.5) / evidence\n",
    "    f_post_prob = (f_prob * 0.5) / evidence\n",
    "    decision = xr.where(\n",
    "        np.isnan(f_post_prob) | np.isnan(nf_post_prob),\n",
    "        np.nan,\n",
    "        np.greater(f_post_prob, nf_post_prob),\n",
    "    )\n",
    "    return nf_post_prob, f_post_prob, decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "flood_dc[[\"nf_post_prob\", \"f_post_prob\", \"decision\"]] = bayesian_flood_decision(\n",
    "    flood_dc\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": [
    "## Postprocessing\n",
    "\n",
    "We continue by improving our flood map by filtering out observations that we expect to have low sensitivity to flooding based on a predefined set of criteria.\n",
    "\n",
    "These criteria include:\n",
    "* Masking of Exceeding Incidence Angles\n",
    "* Identification of Conflicting Distributions\n",
    "* Removal of Measurement Outliers\n",
    "* Denial of High Uncertainty on Decision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_processing(dc):\n",
    "    dc = dc * np.logical_and(dc.MPLIA >= 27, dc.MPLIA <= 48)\n",
    "    dc = dc * (dc.hbsc > (dc.wbsc + 0.5 * 2.754041))\n",
    "    land_bsc_lower = dc.hbsc - 3 * dc.STD\n",
    "    land_bsc_upper = dc.hbsc + 3 * dc.STD\n",
    "    water_bsc_upper = dc.wbsc + 3 * 2.754041\n",
    "    mask_land_outliers = np.logical_and(\n",
    "        dc.sig0 > land_bsc_lower, dc.sig0 < land_bsc_upper\n",
    "    )\n",
    "    mask_water_outliers = dc.sig0 < water_bsc_upper\n",
    "    dc = dc * (mask_land_outliers | mask_water_outliers)\n",
    "    return (dc * (dc.f_post_prob > 0.8)).decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "flood_output = post_processing(flood_dc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {},
   "source": [
    "## Removal of Speckles\n",
    "\n",
    "The following step is designed to further improve the clarity of the floodmaps. These filters do not directly relate to prior knowledge on backscattering, but consists of contextual evidence that supports, or oppose, a flood classification. This mainly targets so-called speckles. These speckles are areas of one or a few pixels, and which are likely the result of the diversity of scattering surfaces at a sub-pixel level. In this approach it is argued that small, solitary flood surfaces are unlikely. Hence speckles are removed by applying a smoothing filter which consists of a rolling window median along the x and y-axis simultaneously.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "flood_output = (\n",
    "    flood_output.rolling({\"longitude\": 5, \"latitude\": 5}, center=True)\n",
    "    .median(skipna=True)\n",
    "    .persist()\n",
    ")\n",
    "wait(flood_output)\n",
    "flood_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "In the following graphic we superimpose the data on a map and we can move the slider to see which areas become flooded over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "flood_output.hvplot.image(\n",
    "    x=\"longitude\",\n",
    "    y=\"latitude\",\n",
    "    rasterize=True,\n",
    "    geo=True,\n",
    "    tiles=True,\n",
    "    project=True,\n",
    "    cmap=[\"rgba(0, 0, 1, 0.1)\", \"darkred\"],\n",
    "    cticks=[(0, \"non-flood\"), (1, \"flood\")],\n",
    "    frame_height=400,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.close(shutdown=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dask-flood-mapper-wHCDD2k4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
